{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9956fe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import rankdata  # Make sure this is included!\n",
    "import math\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import datetime as dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1429b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>size_grp</th>\n",
       "      <th>cowc_gr1a</th>\n",
       "      <th>oaccruals_at</th>\n",
       "      <th>oaccruals_ni</th>\n",
       "      <th>taccruals_at</th>\n",
       "      <th>taccruals_ni</th>\n",
       "      <th>debt_gr3</th>\n",
       "      <th>fnl_gr1a</th>\n",
       "      <th>...</th>\n",
       "      <th>div12m_me</th>\n",
       "      <th>ebitda_mev</th>\n",
       "      <th>eq_dur</th>\n",
       "      <th>eqnpo_12m</th>\n",
       "      <th>eqnpo_me</th>\n",
       "      <th>eqpo_me</th>\n",
       "      <th>ni_me</th>\n",
       "      <th>ocf_me</th>\n",
       "      <th>sale_me</th>\n",
       "      <th>r_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10006</td>\n",
       "      <td>1963-01-31</td>\n",
       "      <td>large</td>\n",
       "      <td>0.070236</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.478462</td>\n",
       "      <td>-0.392308</td>\n",
       "      <td>-0.430769</td>\n",
       "      <td>0.452528</td>\n",
       "      <td>0.479282</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122528</td>\n",
       "      <td>0.095568</td>\n",
       "      <td>-0.256881</td>\n",
       "      <td>-0.162338</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.147790</td>\n",
       "      <td>-0.465470</td>\n",
       "      <td>0.103591</td>\n",
       "      <td>0.036440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10014</td>\n",
       "      <td>1963-01-31</td>\n",
       "      <td>micro</td>\n",
       "      <td>0.488873</td>\n",
       "      <td>0.444615</td>\n",
       "      <td>0.446154</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.438462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.411602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.127424</td>\n",
       "      <td>-0.143731</td>\n",
       "      <td>-0.409091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064917</td>\n",
       "      <td>-0.469613</td>\n",
       "      <td>0.232044</td>\n",
       "      <td>-0.035593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10102</td>\n",
       "      <td>1963-01-31</td>\n",
       "      <td>large</td>\n",
       "      <td>0.128651</td>\n",
       "      <td>0.349231</td>\n",
       "      <td>0.350769</td>\n",
       "      <td>0.226154</td>\n",
       "      <td>0.309231</td>\n",
       "      <td>0.163571</td>\n",
       "      <td>0.313536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138865</td>\n",
       "      <td>-0.159280</td>\n",
       "      <td>0.168196</td>\n",
       "      <td>-0.461039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.220994</td>\n",
       "      <td>-0.325967</td>\n",
       "      <td>-0.222376</td>\n",
       "      <td>-0.006685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10145</td>\n",
       "      <td>1963-01-31</td>\n",
       "      <td>mega</td>\n",
       "      <td>-0.168985</td>\n",
       "      <td>0.238462</td>\n",
       "      <td>0.226154</td>\n",
       "      <td>0.349231</td>\n",
       "      <td>0.344615</td>\n",
       "      <td>-0.207946</td>\n",
       "      <td>-0.207182</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009028</td>\n",
       "      <td>-0.220222</td>\n",
       "      <td>0.279817</td>\n",
       "      <td>-0.478896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.299724</td>\n",
       "      <td>-0.218232</td>\n",
       "      <td>-0.317680</td>\n",
       "      <td>-0.036420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10153</td>\n",
       "      <td>1963-01-31</td>\n",
       "      <td>large</td>\n",
       "      <td>-0.376217</td>\n",
       "      <td>-0.256923</td>\n",
       "      <td>-0.210769</td>\n",
       "      <td>-0.260000</td>\n",
       "      <td>-0.172308</td>\n",
       "      <td>-0.096491</td>\n",
       "      <td>-0.070442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221410</td>\n",
       "      <td>-0.336565</td>\n",
       "      <td>-0.481651</td>\n",
       "      <td>0.405844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.277624</td>\n",
       "      <td>0.088398</td>\n",
       "      <td>0.338398</td>\n",
       "      <td>-0.024319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487003</th>\n",
       "      <td>93426</td>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>small</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.185551</td>\n",
       "      <td>0.202183</td>\n",
       "      <td>0.139719</td>\n",
       "      <td>0.141966</td>\n",
       "      <td>0.318271</td>\n",
       "      <td>-0.105762</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>0.198782</td>\n",
       "      <td>-0.103350</td>\n",
       "      <td>0.041766</td>\n",
       "      <td>-0.093366</td>\n",
       "      <td>-0.227869</td>\n",
       "      <td>0.153088</td>\n",
       "      <td>0.079917</td>\n",
       "      <td>0.120158</td>\n",
       "      <td>-0.050705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487004</th>\n",
       "      <td>93427</td>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>large</td>\n",
       "      <td>0.375254</td>\n",
       "      <td>0.370062</td>\n",
       "      <td>0.345114</td>\n",
       "      <td>0.340135</td>\n",
       "      <td>0.300832</td>\n",
       "      <td>-0.398225</td>\n",
       "      <td>-0.200753</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.053451</td>\n",
       "      <td>0.083755</td>\n",
       "      <td>0.081543</td>\n",
       "      <td>0.056281</td>\n",
       "      <td>-0.013115</td>\n",
       "      <td>0.069279</td>\n",
       "      <td>-0.075246</td>\n",
       "      <td>0.012934</td>\n",
       "      <td>-0.042282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487005</th>\n",
       "      <td>93434</td>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>micro</td>\n",
       "      <td>-0.448941</td>\n",
       "      <td>-0.377339</td>\n",
       "      <td>-0.098233</td>\n",
       "      <td>-0.340915</td>\n",
       "      <td>-0.063703</td>\n",
       "      <td>0.274955</td>\n",
       "      <td>0.195302</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.405277</td>\n",
       "      <td>0.441530</td>\n",
       "      <td>-0.372713</td>\n",
       "      <td>-0.437582</td>\n",
       "      <td>-0.096995</td>\n",
       "      <td>-0.456668</td>\n",
       "      <td>-0.426310</td>\n",
       "      <td>0.310213</td>\n",
       "      <td>0.338963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487006</th>\n",
       "      <td>93436</td>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>mega</td>\n",
       "      <td>-0.318103</td>\n",
       "      <td>-0.224532</td>\n",
       "      <td>-0.091476</td>\n",
       "      <td>0.188329</td>\n",
       "      <td>0.169527</td>\n",
       "      <td>-0.409950</td>\n",
       "      <td>-0.399040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.179026</td>\n",
       "      <td>0.252528</td>\n",
       "      <td>-0.310925</td>\n",
       "      <td>-0.186602</td>\n",
       "      <td>-0.381967</td>\n",
       "      <td>-0.070835</td>\n",
       "      <td>-0.085106</td>\n",
       "      <td>-0.353505</td>\n",
       "      <td>-0.370713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2487007</th>\n",
       "      <td>101801701</td>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>micro</td>\n",
       "      <td>-0.008848</td>\n",
       "      <td>0.241424</td>\n",
       "      <td>0.243763</td>\n",
       "      <td>0.248895</td>\n",
       "      <td>0.264171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.016221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.293234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.475603</td>\n",
       "      <td>-0.360277</td>\n",
       "      <td>-0.381967</td>\n",
       "      <td>-0.193565</td>\n",
       "      <td>-0.236637</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.083846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2487008 rows Ã— 135 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id       date size_grp  cowc_gr1a  oaccruals_at  oaccruals_ni  \\\n",
       "0            10006 1963-01-31    large   0.070236      0.420000      0.478462   \n",
       "1            10014 1963-01-31    micro   0.488873      0.444615      0.446154   \n",
       "2            10102 1963-01-31    large   0.128651      0.349231      0.350769   \n",
       "3            10145 1963-01-31     mega  -0.168985      0.238462      0.226154   \n",
       "4            10153 1963-01-31    large  -0.376217     -0.256923     -0.210769   \n",
       "...            ...        ...      ...        ...           ...           ...   \n",
       "2487003      93426 2022-11-30    small   0.000435      0.185551      0.202183   \n",
       "2487004      93427 2022-11-30    large   0.375254      0.370062      0.345114   \n",
       "2487005      93434 2022-11-30    micro  -0.448941     -0.377339     -0.098233   \n",
       "2487006      93436 2022-11-30     mega  -0.318103     -0.224532     -0.091476   \n",
       "2487007  101801701 2022-11-30    micro  -0.008848      0.241424      0.243763   \n",
       "\n",
       "         taccruals_at  taccruals_ni  debt_gr3  fnl_gr1a  ...  div12m_me  \\\n",
       "0           -0.392308     -0.430769  0.452528  0.479282  ...  -0.122528   \n",
       "1            0.423077      0.438462  0.000000  0.411602  ...  -0.500000   \n",
       "2            0.226154      0.309231  0.163571  0.313536  ...   0.138865   \n",
       "3            0.349231      0.344615 -0.207946 -0.207182  ...  -0.009028   \n",
       "4           -0.260000     -0.172308 -0.096491 -0.070442  ...   0.221410   \n",
       "...               ...           ...       ...       ...  ...        ...   \n",
       "2487003      0.139719      0.141966  0.318271 -0.105762  ...  -0.500000   \n",
       "2487004      0.340135      0.300832 -0.398225 -0.200753  ...  -0.500000   \n",
       "2487005     -0.340915     -0.063703  0.274955  0.195302  ...  -0.500000   \n",
       "2487006      0.188329      0.169527 -0.409950 -0.399040  ...  -0.500000   \n",
       "2487007      0.248895      0.264171  0.000000 -0.016221  ...   0.000000   \n",
       "\n",
       "         ebitda_mev    eq_dur  eqnpo_12m  eqnpo_me   eqpo_me     ni_me  \\\n",
       "0          0.095568 -0.256881  -0.162338  0.000000  0.000000 -0.147790   \n",
       "1         -0.127424 -0.143731  -0.409091  0.000000  0.000000  0.064917   \n",
       "2         -0.159280  0.168196  -0.461039  0.000000  0.000000 -0.220994   \n",
       "3         -0.220222  0.279817  -0.478896  0.000000  0.000000 -0.299724   \n",
       "4         -0.336565 -0.481651   0.405844  0.000000  0.000000 -0.277624   \n",
       "...             ...       ...        ...       ...       ...       ...   \n",
       "2487003    0.198782 -0.103350   0.041766 -0.093366 -0.227869  0.153088   \n",
       "2487004   -0.053451  0.083755   0.081543  0.056281 -0.013115  0.069279   \n",
       "2487005   -0.405277  0.441530  -0.372713 -0.437582 -0.096995 -0.456668   \n",
       "2487006   -0.179026  0.252528  -0.310925 -0.186602 -0.381967 -0.070835   \n",
       "2487007   -0.293234  0.000000  -0.475603 -0.360277 -0.381967 -0.193565   \n",
       "\n",
       "           ocf_me   sale_me       r_1  \n",
       "0       -0.465470  0.103591  0.036440  \n",
       "1       -0.469613  0.232044 -0.035593  \n",
       "2       -0.325967 -0.222376 -0.006685  \n",
       "3       -0.218232 -0.317680 -0.036420  \n",
       "4        0.088398  0.338398 -0.024319  \n",
       "...           ...       ...       ...  \n",
       "2487003  0.079917  0.120158 -0.050705  \n",
       "2487004 -0.075246  0.012934 -0.042282  \n",
       "2487005 -0.426310  0.310213  0.338963  \n",
       "2487006 -0.085106 -0.353505 -0.370713  \n",
       "2487007 -0.236637 -0.500000 -0.083846  \n",
       "\n",
       "[2487008 rows x 135 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data = pd.read_pickle(\"./usa_131_per_size_ranks_False.pkl\")\n",
    "stock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98e5e14",
   "metadata": {},
   "source": [
    "## defining transformer structure \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ac65e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, D, H):\n",
    "        super().__init__()\n",
    "        self.H = H\n",
    "        self.W = nn.ParameterList([nn.Parameter(torch.randn(D, D)/100) for _ in range(H)])\n",
    "        self.V = nn.ParameterList([nn.Parameter(torch.randn(D, D)/100) for _ in range(H)])\n",
    "\n",
    "    def forward(self, X):  # X: [N_t, D]\n",
    "        heads = []\n",
    "        for h in range(self.H):\n",
    "            \n",
    "            scores = X @ self.W[h] @ X.T/(X.shape[1] ** 0.5)          # [N_t, N_t]\n",
    "            weights = F.softmax(scores, dim=1) + 1e-8      # softmax row-wise\n",
    "            A_h = weights @ X @ self.V[h]           # [N_t, D]\n",
    "            heads.append(A_h)\n",
    "        return sum(heads)                           # [N_t, D]\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, D, dF):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(D, dF)\n",
    "        self.fc2 = nn.Linear(dF, D)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, X):  # X: [N_t, D]\n",
    "\n",
    "        return self.dropout(self.fc2(F.relu(self.fc1(X))))  # [N_t, D]\n",
    "\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, D, H):\n",
    "#         super().__init__()\n",
    "#         self.D = D\n",
    "#         self.H = H\n",
    "#         self.d_proj = D * H\n",
    "\n",
    "#         # Q, K, V linear projections (shared dimensions)\n",
    "#         self.W_q = nn.Linear(D, self.d_proj)\n",
    "#         self.W_k = nn.Linear(D, self.d_proj)\n",
    "#         self.W_v = nn.Linear(D, self.d_proj)\n",
    "\n",
    "#         # optional final projection (can keep or skip)\n",
    "#         self.out_proj = nn.Linear(self.d_proj, D)\n",
    "\n",
    "#     def forward(self, X):  # X: [N_t, D]\n",
    "#         N = X.size(0)\n",
    "\n",
    "#         Q = self.W_q(X).view(N, self.H, self.D)  # [N, H, D]\n",
    "#         K = self.W_k(X).view(N, self.H, self.D)\n",
    "#         V = self.W_v(X).view(N, self.H, self.D)\n",
    "\n",
    "#         attn_scores = torch.einsum('nhd,mhd->nhm', Q, K) / (self.D ** 0.5)  # [N, H, N]\n",
    "#         attn_weights = torch.softmax(attn_scores, dim=-1)  # attention across assets\n",
    "\n",
    "#         context = torch.einsum('nhm,mhd->nhd', attn_weights, V)  # [N, H, D]\n",
    "#         context = context.reshape(N, -1)  # [N, H*D]\n",
    "\n",
    "#         return self.out_proj(context)  # [N, D]\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, D, H, dF):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(D, H)\n",
    "        self.ffn = FeedForward(D, dF)\n",
    "        self.norm1 = nn.LayerNorm(D)\n",
    "        self.norm2 = nn.LayerNorm(D)\n",
    "\n",
    "    def forward(self, X):  # X: [N_t, D]\n",
    "        X = self.norm1(X + self.attn(X))  # normalize after attention residual\n",
    "        X = self.norm2(X + self.ffn(X))   # normalize after FFN residual\n",
    "        return X\n",
    "\n",
    "    \n",
    "class NonlinearPortfolioForward(nn.Module):\n",
    "    def __init__(self, D, K, H=1, dF=256):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(D, H, dF) for _ in range(K)])\n",
    "        self.lambda_out = nn.Parameter(torch.randn(D, 1)/1000)  # final projection\n",
    "\n",
    "    def forward(self, X):  # X: [N_t, D]\n",
    "        for block in self.blocks:\n",
    "            X = block(X)  # propagate through K blocks\n",
    "        w_t = X @ self.lambda_out# [N_t, 1]\n",
    "\n",
    "        return w_t.squeeze()/w_t.sum()       # [N_t]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b030251",
   "metadata": {},
   "source": [
    "## fitting one datapoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c60ae438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_t:\n",
      "\n",
      "         cowc_gr1a  oaccruals_at  oaccruals_ni  taccruals_at  taccruals_ni  \\\n",
      "2398352   0.053532     -0.074965     -0.232962     -0.123539     -0.252922   \n",
      "2398353   0.428728      0.345619      0.366759      0.255704      0.284919   \n",
      "2398354   0.020879      0.140612      0.041029      0.066778     -0.004452   \n",
      "2398355  -0.369388     -0.247149     -0.290264     -0.257095     -0.299666   \n",
      "2398356   0.062637      0.210709      0.233241      0.079577      0.136338   \n",
      "...            ...           ...           ...           ...           ...   \n",
      "2401944   0.000000      0.383171      0.447983      0.444352      0.496105   \n",
      "2401945  -0.494035     -0.456885     -0.212378     -0.386199     -0.107123   \n",
      "2401946  -0.083046      0.331433      0.123366     -0.460490     -0.485810   \n",
      "2401947  -0.050706      0.253547      0.217107      0.235949      0.226767   \n",
      "2401948   0.000000      0.359249      0.296106      0.351141      0.415971   \n",
      "\n",
      "         debt_gr3  fnl_gr1a  ncol_gr1a  nfna_gr1a    noa_at  ...   debt_me  \\\n",
      "2398352  0.466163  0.253547   0.178784  -0.172601  0.108299  ... -0.322915   \n",
      "2398353  0.496040  0.002643  -0.055939   0.014047  0.392801  ... -0.146818   \n",
      "2398354 -0.052916  0.089152  -0.283453  -0.069124 -0.088494  ... -0.059616   \n",
      "2398355  0.008999  0.279972   0.271545  -0.247427  0.236246  ...  0.282983   \n",
      "2398356 -0.039237  0.360640   0.175650  -0.107232 -0.209840  ...  0.115070   \n",
      "...           ...       ...        ...        ...       ...  ...       ...   \n",
      "2401944 -0.221022  0.083866   0.000000   0.430181  0.000000  ...  0.127205   \n",
      "2401945 -0.446004  0.121697  -0.126763   0.206815 -0.397517  ... -0.412093   \n",
      "2401946  0.435205  0.491933   0.366186  -0.487204  0.490883  ...  0.445816   \n",
      "2401947  0.000000 -0.101113  -0.088844   0.093602  0.428953  ... -0.500000   \n",
      "2401948 -0.421886 -0.235744   0.000000   0.190960  0.000000  ...  0.065684   \n",
      "\n",
      "         div12m_me  ebitda_mev    eq_dur  eqnpo_12m  eqnpo_me   eqpo_me  \\\n",
      "2398352   0.124380   -0.090871  0.044300   0.146398  0.062360  0.041519   \n",
      "2398353  -0.500000   -0.106273  0.161238   0.015274 -0.147511 -0.393404   \n",
      "2398354  -0.500000    0.204004 -0.127687   0.165418  0.152405  0.138693   \n",
      "2398355  -0.500000    0.069868  0.000000  -0.167435  0.006711 -0.051531   \n",
      "2398356   0.140262    0.146878  0.197394   0.468012  0.457494  0.452886   \n",
      "...            ...         ...       ...        ...       ...       ...   \n",
      "2401944   0.014791   -0.039345 -0.069381  -0.369741  0.055089 -0.016784   \n",
      "2401945   0.000000   -0.316298  0.360586  -0.450432 -0.322427  0.000000   \n",
      "2401946   0.163292   -0.103752 -0.212378   0.336599  0.224553  0.174912   \n",
      "2401947   0.000000   -0.259731  0.000000  -0.408646 -0.282159 -0.393404   \n",
      "2401948   0.000000    0.442313 -0.363192  -0.249280 -0.270694 -0.254122   \n",
      "\n",
      "            ni_me    ocf_me   sale_me  \n",
      "2398352  0.023915 -0.075640 -0.042456  \n",
      "2398353  0.109288 -0.130979  0.147734  \n",
      "2398354  0.281424  0.139878  0.339644  \n",
      "2398355  0.255284  0.326196  0.307803  \n",
      "2398356  0.291991  0.062013 -0.190476  \n",
      "...           ...       ...       ...  \n",
      "2401944 -0.012236 -0.229700 -0.276822  \n",
      "2401945 -0.178532 -0.143493 -0.062536  \n",
      "2401946  0.171580 -0.025862 -0.208835  \n",
      "2401947 -0.079811 -0.245829 -0.500000  \n",
      "2401948  0.417408  0.158509 -0.034423  \n",
      "\n",
      "[3597 rows x 131 columns]\n",
      "R_t+1\n",
      "              r_1\n",
      "2398352 -0.017488\n",
      "2398353  0.203800\n",
      "2398354 -0.016540\n",
      "2398355 -0.068259\n",
      "2398356 -0.062189\n",
      "...           ...\n",
      "2401944  0.000217\n",
      "2401945 -0.102256\n",
      "2401946  0.086815\n",
      "2401947 -0.175360\n",
      "2401948  0.118835\n",
      "\n",
      "[3597 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "X_t = stock_data[stock_data[\"date\"] == pd.Timestamp(\"2020-12-31\")].drop(columns = [\"r_1\", \"id\", \"date\", \"size_grp\"])\n",
    "R_t_plus_one = stock_data[stock_data[\"date\"] == pd.Timestamp(\"2020-12-31\")][[\"r_1\"]]\n",
    "print(\"X_t:\\n\")\n",
    "print(X_t)\n",
    "print(\"R_t+1\")\n",
    "print(R_t_plus_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f834cbc9",
   "metadata": {},
   "source": [
    "## Training loop -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b64dc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  month 1970-07-31 00:00:00  loss=0.961597 return=0.019389683380723\n",
      "  month 1970-08-31 00:00:00  loss=0.820427 return=0.09422603249549866\n",
      "  month 1970-09-30 00:00:00  loss=1.120282 return=-0.058433741331100464\n",
      "  month 1970-10-31 00:00:00  loss=0.962096 return=0.019134849309921265\n",
      "  month 1970-11-30 00:00:00  loss=0.814618 return=0.0974380299448967\n",
      "  month 1970-12-31 00:00:00  loss=0.799706 return=0.10573747754096985\n",
      "  month 1971-01-31 00:00:00  loss=0.932178 return=0.034506458789110184\n",
      "  month 1971-02-28 00:00:00  loss=0.898970 return=0.05185979604721069\n",
      "  month 1971-03-31 00:00:00  loss=0.940036 return=0.03044569306075573\n",
      "  month 1971-04-30 00:00:00  loss=1.102336 return=-0.04992186278104782\n",
      "  month 1971-05-31 00:00:00  loss=1.034944 return=-0.017321985214948654\n",
      "  month 1971-06-30 00:00:00  loss=1.107538 return=-0.05239606648683548\n",
      "  month 1971-07-31 00:00:00  loss=0.907591 return=0.04732440784573555\n",
      "  month 1971-08-31 00:00:00  loss=0.991711 return=0.004153154324740171\n",
      "  month 1971-09-30 00:00:00  loss=1.116490 return=-0.056641146540641785\n",
      "  month 1971-10-31 00:00:00  loss=1.051050 return=-0.025207407772541046\n",
      "  month 1971-11-30 00:00:00  loss=0.759259 return=0.12864536046981812\n",
      "  month 1971-12-31 00:00:00  loss=0.906355 return=0.047973282635211945\n",
      "  month 1972-01-31 00:00:00  loss=0.897550 return=0.05260899290442467\n",
      "  month 1972-02-29 00:00:00  loss=0.958113 return=0.021167386323213577\n",
      "  month 1972-03-31 00:00:00  loss=0.982833 return=0.00862087868154049\n",
      "  month 1972-04-30 00:00:00  loss=0.964674 return=0.01782183349132538\n",
      "  month 1972-05-31 00:00:00  loss=1.053553 return=-0.0264272503554821\n",
      "  month 1972-06-30 00:00:00  loss=1.095539 return=-0.04668019711971283\n",
      "  month 1972-07-31 00:00:00  loss=1.469400 return=-0.2121879756450653\n",
      "  month 1972-08-31 00:00:00  loss=0.627247 return=0.20801082253456116\n",
      "  month 1972-09-30 00:00:00  loss=0.494852 return=0.29654309153556824\n",
      "  month 1972-10-31 00:00:00  loss=0.223634 return=0.5271006226539612\n",
      "  month 1972-11-30 00:00:00  loss=10.321612 return=4.212726593017578\n",
      "  month 1972-12-31 00:00:00  loss=1.452015 return=-0.20499582588672638\n",
      "  month 1973-01-31 00:00:00  loss=1.303962 return=-0.14191167056560516\n",
      "  month 1973-02-28 00:00:00  loss=1.130013 return=-0.06302060186862946\n",
      "  month 1973-03-31 00:00:00  loss=1.181298 return=-0.08687514066696167\n",
      "  month 1973-04-30 00:00:00  loss=1.092308 return=-0.04513521119952202\n",
      "  month 1973-05-31 00:00:00  loss=1.085468 return=-0.04185802489519119\n",
      "  month 1973-06-30 00:00:00  loss=0.751033 return=0.13337832689285278\n",
      "  month 1973-07-31 00:00:00  loss=1.050431 return=-0.02490546740591526\n",
      "  month 1973-08-31 00:00:00  loss=0.855118 return=0.07527430355548859\n",
      "  month 1973-09-30 00:00:00  loss=1.008364 return=-0.004173102788627148\n",
      "  month 1973-10-31 00:00:00  loss=1.445599 return=-0.20233085751533508\n",
      "  month 1973-11-30 00:00:00  loss=0.997713 return=0.001144319074228406\n",
      "  month 1973-12-31 00:00:00  loss=0.938227 return=0.031378988176584244\n",
      "  month 1974-01-31 00:00:00  loss=0.978002 return=0.011060032993555069\n",
      "  month 1974-02-28 00:00:00  loss=0.986821 return=0.006611501798033714\n",
      "  month 1974-03-31 00:00:00  loss=1.062142 return=-0.030602728947997093\n",
      "  month 1974-04-30 00:00:00  loss=1.070525 return=-0.034661613404750824\n",
      "  month 1974-05-31 00:00:00  loss=1.000238 return=-0.00011879962403327227\n",
      "  month 1974-06-30 00:00:00  loss=1.081834 return=-0.040112562477588654\n",
      "  month 1974-07-31 00:00:00  loss=1.144513 return=-0.06981933116912842\n",
      "  month 1974-08-31 00:00:00  loss=1.120900 return=-0.05872563645243645\n",
      "  month 1974-09-30 00:00:00  loss=0.684995 return=0.17235589027404785\n",
      "  month 1974-10-31 00:00:00  loss=0.937070 return=0.03197648376226425\n",
      "  month 1974-11-30 00:00:00  loss=0.747821 return=0.13523364067077637\n",
      "  month 1974-12-31 00:00:00  loss=31.422403 return=-4.605568885803223\n",
      "  month 1975-01-31 00:00:00  loss=1.555359 return=-0.24714040756225586\n",
      "  month 1975-02-28 00:00:00  loss=0.653643 return=0.19151802361011505\n",
      "  month 1975-03-31 00:00:00  loss=1.210098 return=-0.1000446304678917\n",
      "  month 1975-04-30 00:00:00  loss=0.784249 return=0.114421546459198\n",
      "  month 1975-05-31 00:00:00  loss=0.808914 return=0.10060364007949829\n",
      "  month 1975-06-30 00:00:00  loss=1.125762 return=-0.06101911514997482\n"
     ]
    }
   ],
   "source": [
    "months_list = stock_data[\"date\"].unique()\n",
    "columns_to_drop_in_x = [\"size_grp\", \"date\", \"r_1\", \"id\"]\n",
    "window = 60\n",
    "epoch = 1\n",
    "K = 2\n",
    "D = stock_data.shape[1] - len(columns_to_drop_in_x)\n",
    "H = 2\n",
    "dF = 64\n",
    "t = 150  # --> 61st month in the months_list\n",
    "\n",
    "lr = 1e-4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = NonlinearPortfolioForward(D=D, K=K, H=H, dF=dF).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "losses = []\n",
    "returns = []\n",
    "\n",
    "for month in months_list[t - window:t]:  # this loop iterates until t-1\n",
    "    month_data = stock_data[stock_data[\"date\"] == month]\n",
    "\n",
    "    X_t = month_data.drop(columns=columns_to_drop_in_x)\n",
    "\n",
    "    R_t_plus_one = torch.tensor(\n",
    "        month_data[\"r_1\"].values,\n",
    "        dtype=torch.float32,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    X_t_tensor = torch.tensor(X_t.values, dtype=torch.float32, device=device)  # Shape: [N_t, D]\n",
    "    w_t = model(X_t_tensor)  # Shape: [N_t]\n",
    "\n",
    "    loss = (1 - torch.dot(w_t, R_t_plus_one)).pow(2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if torch.isnan(param.grad).any():\n",
    "            print(f\"NaN detected in gradient of {name}\")\n",
    "            break\n",
    "\n",
    "    portfolio_return = torch.dot(w_t, R_t_plus_one).item()\n",
    "    losses.append(loss.item())\n",
    "    returns.append(portfolio_return)\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"  month {month}  loss={loss.item():.6f} return={w_t @ R_t_plus_one}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeb47f6",
   "metadata": {},
   "source": [
    "### now it has been trained from t-60 to t-1, time to give him informations at t and get the weights w_t.\n",
    "To see if it fucking worked than do w_t @ R_t_plus_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11a1497c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0989, grad_fn=<DotBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_data = stock_data[stock_data[\"date\"] == months_list[t]]\n",
    "\n",
    "X_t = month_data.drop(columns=columns_to_drop_in_x)\n",
    "\n",
    "R_t_plus_one = torch.tensor(\n",
    "    month_data[\"r_1\"].values,\n",
    "    dtype=torch.float32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "X_t_tensor = torch.tensor(X_t.values, dtype=torch.float32, device=device)  # Shape: [N_t, D]\n",
    "w_t = model(X_t_tensor)  # Shape: [N_t]\n",
    "w_t@R_t_plus_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea80c4b",
   "metadata": {},
   "source": [
    "# NOW LET'S DO THIS FOR BUNCH OF t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cecf5612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979-09-30 00:00:00\n",
      "-0.08768456429243088\n",
      "1979-10-31 00:00:00\n",
      "0.6532599329948425\n",
      "1979-11-30 00:00:00\n",
      "0.12158513069152832\n",
      "1979-12-31 00:00:00\n",
      "0.07817015796899796\n",
      "1980-01-31 00:00:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-faa87cf1dd69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             )\n\u001b[1;32m--> 487\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "portfolio_ret = []\n",
    "for t in range(200, len(months_list)):\n",
    "    model = NonlinearPortfolioForward(D=D, K=K, H=H, dF=dF).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "    losses = []\n",
    "    returns = []\n",
    "    print(months_list[t])\n",
    "\n",
    "    for month in months_list[t - window:t]:  # this loop iterates until t-1\n",
    "        month_data = stock_data[stock_data[\"date\"] == month]\n",
    "\n",
    "        X_t = month_data.drop(columns=columns_to_drop_in_x)\n",
    "\n",
    "        R_t_plus_one = torch.tensor(\n",
    "            month_data[\"r_1\"].values,\n",
    "            dtype=torch.float32,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        X_t_tensor = torch.tensor(X_t.values, dtype=torch.float32, device=device)  # Shape: [N_t, D]\n",
    "        w_t = model(X_t_tensor)  # Shape: [N_t]\n",
    "\n",
    "        loss = (1 - torch.dot(w_t, R_t_plus_one)).pow(2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        portfolio_return = torch.dot(w_t, R_t_plus_one).item()\n",
    "        losses.append(loss.item())\n",
    "        returns.append(portfolio_return)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        #print(f\"  month {month}  loss={loss.item():.6f} return={w_t @ R_t_plus_one}\")\n",
    "    month_data = stock_data[stock_data[\"date\"] == months_list[t]]\n",
    "\n",
    "    X_t = month_data.drop(columns=columns_to_drop_in_x)\n",
    "\n",
    "    R_t_plus_one = torch.tensor(\n",
    "        month_data[\"r_1\"].values,\n",
    "        dtype=torch.float32,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    X_t_tensor = torch.tensor(X_t.values, dtype=torch.float32, device=device)  # Shape: [N_t, D]\n",
    "    w_t = model(X_t_tensor)  # Shape: [N_t]\n",
    "\n",
    "    predicted = (w_t @ R_t_plus_one).item()\n",
    "    portfolio_ret.append(predicted)\n",
    "    print(predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6d878af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cumulative_return = np.cumsum(np.asarray(portfolio_ret))\n",
    "\n",
    "plt.plot(cumulative_return)\n",
    "plt.title(\"Cumulative Portfolio Return\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Cumulative Return\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
